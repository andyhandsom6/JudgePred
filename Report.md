# 刑罚预测模型
*李安齐*

*北京大学元培学院，2300017432*

## 一、方法

本次实验，我们选择了一个思路简单清晰的方法：通过对基座大模型进行监督微调（SFT），让模型获得预测罪名、刑期的能力，从而解决刑罚预测问题。

选择这个方法，主要是因为考虑到我们面临的挑战：
- 若直接zero-shot地提示大模型进行预测，难以充分利用训练数据；
- 且上述方法耗时长、成本高昂；

考虑到对于Task1和Task2，分别有较长的罪名文件`charges.json`与法律条文`articles.json`，在面临上述挑战的情况下，与其费办法把它们通过某种方式塞入提示词，不如直接通过SFT，令模型隐式地学习到这些内容。

于是，我们对Task1和Task2提出通用的方法：将训练数据中的犯罪事实、被告人信息写成结构化的提示词，作为训练的输入；将训练数据的`outcomes`一栏作为监督信号，令模型直接输出json格式的判罚结果。

## 二、实现

### 基线模型实现
我们采用Qwen3-0.6B作为基座模型，在一个有8张A100的远程服务器上训练大约15分钟，得到我们的模型。模型的超参数如下：

| 参数             | 值     |
|------------------|--------|
| 批规模       | 1      |
| 梯度积累步数 | 8      |
| 学习率    | 5e-5   |
| epoch           | 3      |
| 最大输入token       | 5120   |
| 参数精度 | torch.bfloat16 |

超参数没有经过任何精细的调整。这可能是改进模型能力的一个方向。同时，由于受到显存限制，采用了不够合理的批规模（经验上看，过小）。

对于实验数据，我们首先进行结构化的预处理。通过以下代码构造提示词：
```python
def create_prompt(example, charges):
    fact = example["fact"]
    defendants = example["defendants"]
    prompt = f"""【案件事实】
{fact}
【被告人信息】
{defendants}
"请根据上述案件事实，直接以json的形式给出每名被告人的罪名和刑期（单位：月）。"
"""
    return prompt
```

然后，我们利用`accelerate`包调度多卡进行训练。我们按照Qwen3官方文档的引导，通过构造token序列，关闭了Qwen3的深度思考功能。
更多的细节可以在代码中找到，没有值得讨论的部分。

### 改进1：数据增广

当前的方法，对于一个人多个罪名的情况，如果预测错了顺序，是会被loss惩罚的。然而实际的metric，是对顺序没有要求的（事实上也没有顺序）。所以我们采取一个简单的数据增广：每次训练代码收集数据时，随机打乱一个人的罪名顺序。这样，可以让模型学到罪名的顺序无关性。


### 改进2：长文本压缩

我们观察到，超过30%的实验数据在训练中由于分词后token数量超过阈值，会被截断。一个可行的方法，是预先用LLM处理数据，对过长的文本进行概括，将其token数量降低至阈值以下。

### 改进3：学习法条知识

对于Task2，我们希望模型可以充分利用法条知识。所以我们在训练数据中加入法条知识，让模型对它充分掌握。




## 三、实验

我们直接展示消融实验的结果：

| 模型 | 任务1 | 任务2 |
| --- | --- | --- |
| 基线 |  0.823 |0.169|
|+改进1|  |  |
|+改进2|  |  |
|+改进3|  |  |


## 四、分析



此外，在实验过程中，我们还观察到一些现象：
对于Task2，如果我们不在提示词里明确指出“请预测罪名和**刑期**”，模型有的时候不能正常地输出json格式的结果，而是会输出不符合格式要求的字符。这本来不应该出现，因为SFT理应让模型学会给出严格符合格式的输出要求。我们推测，这种现象的原因可能是：
- 训练样本太少
- epoch、lr等超参数设置不够合理

导致模型学得不够好、或者不够充分。